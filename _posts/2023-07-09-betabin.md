---
title: Bayesian Inference with Binomial Data
layout: post
date: 2023-07-09
tags: bayesian statistics 
published: true
---

The binomial distribution is a discrete probability distribution that models the
number of successes $$x$$ in a fixed number of independent trials $$n$$, each
with the same probability of success $$p$$. The probability mass function (PMF)
of the binomial distribution takes the following form:

$$\begin{align}
P(x\vert n,p) \sim Bin(x\vert n,p)&={n \choose x}p^{x}(1-p)^{n-x} \\
\text{with kernel }
ker[Bin(x\vert n,p)] & = p^{x}(1-p)^{n-x} \\ 
\end{align}$$

It is often the case when investigating binomial systems that the underlying
probability of success $$p$$ is unknown such that we must infer possible values
of $$p$$ from observed data. 

### Modeling $$p$$ with the Beta Distribution
The beta distribution is a continuous probability distribution defined
on the interval $$[0,1]$$ that we may use to model our uncertainty regarding the
probability of success $$p$$. The beta distribution has the following
probability density function (PDF):

$$\begin{align}
P(p\vert\alpha,\beta) \sim beta(p\vert\alpha,\beta)&=\frac{p^{\alpha-1}(1-p)^{\beta-1}}{B(\alpha,\beta)}\\
\text{with kernel }ker[Beta(p\vert\alpha,\beta)]&=p^{\alpha-1}(1-p)^{\beta-1}
\end{align}$$

where $$\alpha$$ and $$\beta$$ are hyperparameters encoding prior certainty or
information and $$B(\alpha,\beta)$$ represents the [Beta
function](https://en.wikipedia.org/wiki/Beta_function) serving as a normalizing
constant.

### Deriving the Posterior Distribution 

We may now derive the kernel of the posterior distribution as the product of the
kernels of the binomial likelihood and beta prior.

$$
\begin{align}
ker[P(p\vert n,x)] & = ker[P(x\vert n,p)]\cdot ker[P(p\vert\alpha,\beta)] \\
& = ker[Bin(x\vert n,p)]\cdot ker[Beta(p\vert\alpha,\beta)] \\
& = p^{x}(1-p)^{n-x}\cdot p^{\alpha-1}(1-p)^{\beta-1} \\
& = p^{x+\alpha-1}(1-p)^{n-x+\beta-1} \\
& = ker[Beta(p\vert x+\alpha,n-x+\beta)] \\
\end{align}
$$

Thus, the posterior distribution is a beta distribution with hyperparameters
$$\alpha'=x+\alpha$$ and $$\beta'=n-x+\beta$$, proving the conjugacy of the
binomial and beta distributions.

After renormalizing, the posterior distribution takes the following probability
density function:

$$P(p\vert\textbf{x}) \sim beta(p\vert\alpha^\prime,\beta^\prime)=\frac{p^{\alpha^\prime-1}(1-p)^{\beta^\prime-1}}{B(\alpha^\prime,\beta^\prime)}$$

### Intuition of the Beta Hyperparameters

The $$\alpha$$ and $$\beta$$ hyperparameters of the beta prior distribution can
seem somewhat cryptic at first, until one recognizes that they represent
pseudo-observations of the prior. A beta prior with hyperparameters $$(1,1)$$ is
used as an uninformative prior as it represents a system where one has observed
no trial successes yet both success and failure are possible (represented by a
single pseudo-observation for both success and failure). Meanwhile a beta prior
of $$(86,16)$$ can be understood as a system in which there have been 100
trials, with 85 successes and 15 failures informing an otherwise uninformed
prior. Thus in the process of updating our beta prior using Bayes' theorem, it
makes sense that the resulting posterior distribution would have parameters
$$\alpha^\prime=x+\alpha$$ and $$\beta^\prime=n-x+\beta$$ as it simply
represents the addition of real observations of the data to the
pseudo-observations of the prior.

### Deriving the Posterior Predictive Distribution (PPD)

Once the data $$\textbf{x}=\{x_1,...,x_n\}$$ has been incorporated into the
prior $$P(p\vert\alpha,\beta)$$ to generate the posterior distribution
$$P(p\vert\alpha^\prime,\beta^\prime)$$, we may then leverage the uncertainty
embedded within it to calculate a posterior predictive distribution
$$P(\tilde{x} \vert \textbf{x})$$. The PPD allows us to predict the probability
of some future value $$\tilde{x}$$ drawn from the same distribution as the data
that we observed, while still respecting our uncertainty about $$p$$.

More formally, the PPD is the distribution of possible unobserved values
$$\tilde{x}$$ conditioned on the set of previously observed values
$$\textbf{x}=\{x_1,...,x_n\}$$ which are drawn from a distribution that depends
on parameter $$\theta\in\Theta$$ where $$\Theta$$ is the parameter space. The
posterior predictive distribution is calculated by marginalizing the likelihood
over the posterior.

$$PPD\stackrel{\text{def}}{=}
P(\tilde{x}|\textbf{x})\stackrel{\text{def}}{=}\int_{\theta}P(x\vert\theta)P(\theta\vert\textbf{x})d\theta$$

In the case of the binomial model, the parameter of interest $$\theta$$ is the
probability of success $$p$$. Rewriting the above and simplifying,

$$\begin{align}
PPD&= P(\tilde{x}|n,\alpha^\prime,\beta^\prime)\\
&=\int_{p}P(x\vert n,p)P(p\vert\alpha^\prime,\beta^\prime)dp \\
& = \int_0^1Bin(x\vert n,p)Beta(p\vert\alpha',\beta')dp \\
& = {n \choose x}\frac{1}{B(\alpha',\beta')}\int_0^{1}p^{x+\alpha'-1}(1-p)^{n-x+\beta'-1}dp \\
& = {n \choose x}\frac{B(x+\alpha',n-x+\beta')}{B(\alpha',\beta')} \\
& = betaBin(n,\alpha',\beta') \\
\end{align}$$

Thus, the posterior predictive distribution follows the beta-Binomial distribution with parameters $$n$$, $$\alpha^\prime$$, and $$\beta^\prime$$.

### Example Application: Amazon Seller Reviews

All of this theory is great and all, but let's see about applying it to a
relevant real-world example: buying something from Amazon. 

Imagine you are buying a book from Amazon and have the option of buying from
three different sellers. Seller 1 has a 100% positive rating with 10 reviews.
Seller 2 has a 96% positive average rating with 50 reviews. Seller 3 has a 92%
positive average rating with 200 reviews. Assuming each seller has an underlying
probability $$p$$ of providing a good experience and a complementary probability
$$(1-p)$$ of providing a poor experience, with which seller will you have the
highest probability of having a good shopping experience?

We can use the Beta-binomial model derived above to answer this question as the
situation can be paramterized as a single trial (i.e. Bernoulli trial) of a
binomially-distributed random variable. Let us leverage what we know about the
conjugacy of the beta distribution to model our certainty in each seller's
probability $$p$$ of providing a good experience.

### Calculating the Posterior Distributions

Assuming each seller to have an underlying beta-distributed probability $$p$$ of
providing a good experience we can calculate the posterior distributions for
each given an uninformative prior $$Beta(\alpha=1,\beta=1)$$ and the data.

$$\begin{align}
P(p_1\vert\textbf{x}_1)&=Beta(p_1\vert\alpha^\prime=11,\beta^\prime=1)\\
P(p_2\vert\textbf{x}_2)&=Beta(p_2\vert\alpha^\prime=49,\beta^\prime=3)\\
P(p_3\vert\textbf{x}_3)&=Beta(p_3\vert\alpha^\prime=185,\beta^\prime=17)\\
\end{align}$$

Let us graph the posterior distributions to get a sense of our relative
certainty in the value of each seller's $$p$$.

{% include figure.html file="betabin_fig1.svg" caption="Our certainty about
\(p\) for each of the three sellers." width="4in" %}

In analyzing the above graph, there is still no clear answer to the original
question, as
- Seller 1 has the highest maximum a posteriori, but there is a lot of
  uncertainty in that value, with the 95% HDI including values down to 0.76
- Seller 2 and Seller 3 have considerable overlap in their posterior densities,
  although it is clear that for almost all values of $$p$$, Seller 2 is at least
  as good, if not better, than Seller 3.

This is okay, as the quantity of interest is actually the expected value of the
random variable; that is, we want to predict what is the most likely future draw
from the same distribution that generated the original values. We can find the
expected value using the posterior predictive distribution. 

### Calculating the Posterior Predictive Distribution

As we derived above, the posterior predictive distribution for the binomial
model is the beta-binomial distribution with the following PMF:

$$\begin{align}
P(\tilde{x}|n,\alpha^\prime,\beta^\prime) &\sim betaBin(n,\alpha',\beta') \\
&= {n \choose x}\frac{B(x+\alpha',n-x+\beta')}{B(\alpha',\beta')}
\end{align}$$

Using the [factorial definition of the beta
function](https://en.wikipedia.org/wiki/Beta_function#Properties), we can
simplify the above equation for our purposes, as we are interested in the
probability that $$\tilde{x}=1$$ (i.e. we have a good experience with the
seller) given $$n=1$$ (in our next single order). Thus,

$$\begin{align}
P(\tilde{x}=1|n=1,\alpha^\prime,\beta^\prime) &\sim betaBin(n=1,\alpha',\beta')\\
&= {1 \choose 1}\frac{B(1+\alpha',1-1+\beta')}{B(\alpha',\beta')} \\
&= \frac{B(1+\alpha',\beta')}{B(\alpha',\beta')} \\
&= \frac{\alpha'}{\alpha'+\beta'}
\end{align}$$

We now calculate the expected value for each of the three sellers.

$$\begin{align}
P(\tilde{x}=1|\textbf{x}_1) 
&= \frac{11}{11+1}\\
&= .9167 \\
&= 91.7\%
\end{align}$$

$$\begin{align}
P(\tilde{x}=1|\textbf{x}_2) 
&= \frac{49}{49+3}\\
&= .9423 \\
&= 94.2\%
\end{align}$$

$$\begin{align}
P(\tilde{x}=1|\textbf{x}_3) 
&= \frac{185}{185+17}\\
&= .9158 \\
&= 91.6\%
\end{align}$$

It is settled! Given the collected data, the seller with the highest probability
of giving a good experience is Seller #2, with an probability of 94.2%. The
other two options, sellers #1 and #3 are nearly identical with probabilities of
91.7% and 91.6% respectively.
